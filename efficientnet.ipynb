{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bbd29b3",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4461ec6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c27c3c4",
   "metadata": {},
   "source": [
    "## Logs\n",
    "##### Function to log the training metrics\n",
    "###### (Change the path name in `log_path` as required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d75ccd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "log_path = \"logs/training4_log.csv\"\n",
    "with open(log_path, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['epoch', 'train_loss', 'train_accuracy', 'val_loss', 'val_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "219a86d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b33aeb7",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dfeba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your transform (modify as needed)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Converts HWC NumPy -> CHW Tensor\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                         std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Load MRI volumes into a list\n",
    "def load_npy_data(root_dir):\n",
    "    samples = []\n",
    "    print(f\"Loading data from {root_dir}...\")\n",
    "    start = time.time()\n",
    "    \n",
    "    for label in os.listdir(root_dir):\n",
    "        label_path = os.path.join(root_dir, label)\n",
    "        if os.path.isdir(label_path):\n",
    "            for file in tqdm(os.listdir(label_path), desc=label):\n",
    "                full_path = os.path.join(label_path, file)\n",
    "                vol = np.load(full_path)  # (20, H, W)\n",
    "                vol = np.repeat(vol[:, :, :, None], 3, axis=3)  # (20, H, W, 3)\n",
    "                \n",
    "                # Apply transforms to each slice\n",
    "                vol_tensor = torch.stack([transform(slice) for slice in vol])  # (20, 3, H, W)\n",
    "                label_val = 0 if label == \"demented\" else 1\n",
    "                samples.append((vol_tensor, label_val))\n",
    "    \n",
    "    print(f\"‚úÖ Loaded {len(samples)} samples in {time.time() - start:.2f}s\")\n",
    "    return samples\n",
    "\n",
    "train_dataset = load_npy_data(r\"D:\\Alzeimers-detection\\dataset\\train\")\n",
    "val_dataset = load_npy_data(r\"D:\\Alzeimers-detection\\dataset\\val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196eb53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d29c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Label Distribution: Counter({0: 65, 1: 62})\n",
      "Validation Set Label Distribution: Counter({0: 17, 1: 16})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Check label distribution in training set\n",
    "train_labels = [label for _, label in train_dataset]\n",
    "val_labels = [label for _, label in val_dataset]\n",
    "\n",
    "print(\"Training Set Label Distribution:\", Counter(train_labels))\n",
    "print(\"Validation Set Label Distribution:\", Counter(val_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8b8bdc",
   "metadata": {},
   "source": [
    "## Load the Model (Efficientnet-B2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ed5d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\omana\\AppData\\Local\\Temp\\ipykernel_26604\\2920788952.py:18: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    }
   ],
   "source": [
    "model = EfficientNet.from_pretrained(\"efficientnet-b2\")\n",
    "feature_dim = model._fc.in_features\n",
    "model._fc = nn.Identity()\n",
    "\n",
    "classifier = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(feature_dim, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.2),\n",
    "    nn.Linear(512, 2)\n",
    ")\n",
    "\n",
    "model = nn.Sequential(model, classifier).to(device)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "scaler = GradScaler()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e096536",
   "metadata": {},
   "source": [
    "## Train from a save\n",
    "##### Run the below cell if you want to train from a saved state. Change the path inside `torch.load()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce7653c",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"checkpoint_epoch_10.pt\")  # or checkpoint_epoch_8.pt\n",
    "model.load_state_dict(checkpoint['model_state'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state'])\n",
    "scaler.load_state_dict(checkpoint['scaler_state'])\n",
    "start_epoch = checkpoint['epoch'] + 1  # Resume from next epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4afbef3",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "###### Increase or decrease the patience accrodingly. The model tends to plateau at the beginning and will suddenely generalize well, after which it will start to overfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949eb09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs=25, start_epoch=1, patience=7):\n",
    "    best_val_acc = 0\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_since_improvement = 0\n",
    "\n",
    "    # Ensure CSV log starts clean\n",
    "    if not os.path.exists(log_path):\n",
    "        with open(log_path, mode='w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['epoch', 'train_loss', 'train_acc', 'val_loss', 'val_acc'])\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_start = time.time()\n",
    "        total_loss, total_correct, total_samples = 0, 0, 0\n",
    "\n",
    "        for x, y in tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs}\"):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            B, S, C, H, W = x.shape\n",
    "            x = x.view(B * S, C, H, W).float()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            with autocast():\n",
    "                features = model[0].extract_features(x)\n",
    "                pooled = nn.AdaptiveAvgPool2d(1)(features).view(B, S, -1)\n",
    "                mean_features = pooled.mean(dim=1)\n",
    "                out = model[1](mean_features)\n",
    "                loss = criterion(out, y)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_correct += (out.argmax(1) == y).sum().item()\n",
    "            total_samples += y.size(0)\n",
    "\n",
    "        train_loss = total_loss / len(train_loader)\n",
    "        train_acc = 100 * total_correct / total_samples\n",
    "        print(f\"\\n[Epoch {epoch}] Train Loss: {train_loss:.4f} | Acc: {train_acc:.2f}% | Time: {time.time() - epoch_start:.1f}s\")\n",
    "\n",
    "        # === Validation ===\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_samples = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                B, S, C, H, W = x.shape\n",
    "                x = x.view(B * S, C, H, W).float()\n",
    "\n",
    "                with autocast():\n",
    "                    features = model[0].extract_features(x)\n",
    "                    pooled = nn.AdaptiveAvgPool2d(1)(features).view(B, S, -1)\n",
    "                    mean_features = pooled.mean(dim=1)\n",
    "                    out = model[1](mean_features)\n",
    "                    loss = criterion(out, y)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                val_correct += (out.argmax(1) == y).sum().item()\n",
    "                val_samples += y.size(0)\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = 100 * val_correct / val_samples\n",
    "        print(f\"           Val Loss:   {val_loss:.4f} | Acc: {val_acc:.2f}%\")\n",
    "\n",
    "        # === Log to CSV ===\n",
    "        with open(log_path, mode='a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([epoch, train_loss, train_acc, val_loss, val_acc])\n",
    "\n",
    "        # === Save Best Accuracy Model ===\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save({'epoch': epoch, 'model_state': model.state_dict()}, 'best_val.pt')\n",
    "            print(\"Best val accuracy model saved.\")\n",
    "            epochs_since_improvement = 0\n",
    "        else:\n",
    "            epochs_since_improvement += 1\n",
    "\n",
    "        # === Save Best Generalization (lowest val loss) ===\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save({'epoch': epoch, 'model_state': model.state_dict()}, 'best_generalization.pt')\n",
    "            print(\"Best generalization model saved.\")\n",
    "\n",
    "        # === Early Stopping Check ===\n",
    "        if epochs_since_improvement >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch} (no improvement for {patience} epochs).\")\n",
    "            break\n",
    "\n",
    "    # === Save Final Model ===\n",
    "    torch.save({'epoch': epoch, 'model_state': model.state_dict()}, 'last_epoch.pt')\n",
    "    print(\"Final model saved as last_epoch.pt\")\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d5a012",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], \n",
    "                         [0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "test_dir = \"dataset/test/\"\n",
    "\n",
    "test_dataset = datasets.ImageFolder(\n",
    "    root=test_dir,\n",
    "    transform=test_transform\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=32,      \n",
    "    shuffle=False,       \n",
    "    num_workers=2,       \n",
    "    pin_memory=True      \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cfa1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = datasets.ImageFolder(\"dataset/test\", transform=test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a50248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b2\n"
     ]
    }
   ],
   "source": [
    "from efficientnet_pytorch import EfficientNet\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# === Recreate the same model used during training ===\n",
    "model = EfficientNet.from_pretrained(\"efficientnet-b2\")\n",
    "feature_dim = model._fc.in_features\n",
    "model._fc = nn.Identity()\n",
    "\n",
    "classifier = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(feature_dim, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.2),\n",
    "    nn.Linear(512, 2)\n",
    ")\n",
    "\n",
    "model = nn.Sequential(model, classifier).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da3669c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating 3 checkpoint(s)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\omana\\AppData\\Local\\Temp\\ipykernel_26604\\2896019465.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(ckpt_path, map_location=device)\n",
      "Evaluating checkpoints/best_val.pt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:08<00:00,  3.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoints/best_val.pt: Test Acc = 36.39%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating checkpoints/best_generalization.pt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:07<00:00,  3.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoints/best_generalization.pt: Test Acc = 54.82%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating checkpoints/last_epoch.pt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:08<00:00,  2.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoints/last_epoch.pt: Test Acc = 33.46%\n",
      "\n",
      "üèÜ Best Checkpoint:\n",
      "checkpoints/best_generalization.pt ‚Üí 54.82%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([('checkpoints/best_val.pt', 36.385836385836384),\n",
       "  ('checkpoints/best_generalization.pt', 54.82295482295483),\n",
       "  ('checkpoints/last_epoch.pt', 33.45543345543345)],\n",
       " 'checkpoints/best_generalization.pt')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_checkpoint(ckpt_path, model, test_loader, device):\n",
    "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state'])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in tqdm(test_loader, desc=f\"Evaluating {ckpt_path}\"):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            out = model(x)\n",
    "            all_preds.append(out.softmax(dim=1).cpu().numpy())\n",
    "            all_labels.append(y.cpu().numpy())\n",
    "\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "\n",
    "    pred_classes = np.argmax(all_preds, axis=1)\n",
    "    acc = (pred_classes == all_labels).mean() * 100\n",
    "\n",
    "    return acc\n",
    "\n",
    "\n",
    "def check_all_checkpoints(model, test_loader, device, ckpt_paths=None):\n",
    "    if ckpt_paths is None:\n",
    "        ckpt_paths = [\n",
    "            \"checkpoints/best_val.pt\",\n",
    "            \"checkpoints/best_generalization.pt\",\n",
    "            \"checkpoints/last_epoch.pt\"\n",
    "        ]\n",
    "\n",
    "    print(f\"Evaluating {len(ckpt_paths)} checkpoint(s)...\")\n",
    "\n",
    "    best_acc = 0\n",
    "    best_ckpt = None\n",
    "    results = []\n",
    "\n",
    "    for ckpt in ckpt_paths:\n",
    "        acc = evaluate_checkpoint(ckpt, model, test_loader, device)\n",
    "        results.append((ckpt, acc))\n",
    "        print(f\"{ckpt}: Test Acc = {acc:.2f}%\")\n",
    "        \n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_ckpt = ckpt\n",
    "\n",
    "    print(\"\\nüèÜ Best Checkpoint:\")\n",
    "    print(f\"{best_ckpt} ‚Üí {best_acc:.2f}%\")\n",
    "\n",
    "    return results, best_ckpt\n",
    "\n",
    "# Run this:\n",
    "check_all_checkpoints(model, test_loader, device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
